{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('ToyotaCorolla.xlsx', sheet_name='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1436, 39)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Diesel', 'Petrol', 'CNG'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Fuel_Type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get dummies for fuel type \n",
    "df = pd.get_dummies(df, columns=['Fuel_Type'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Price']\n",
    "X = df.drop(columns=['Price', 'Color', \n",
    "                     'Id', \"Model\",  \n",
    "                     'Mfg_Month', 'Mfg_Year',\n",
    "                    'Fuel_Type_Petrol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                  test_size = .4,\n",
    "                                                  random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> The process of supervised machine learning</h3>\n",
    "<img src='train_test_process.jpg' >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from statistics import mean\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "regr = LinearRegression()\n",
    "regr.fit(x_train, y_train)\n",
    "y_pred = regr.predict(x_test)\n",
    "print('Average error: %.2f' %mean(y_test - y_pred))\n",
    "print('Mean absolute error: %.2f' %mean_absolute_error(y_test, y_pred))\n",
    "print('Mean absolute error: %.2f' %(mean(abs(y_test - y_pred))))\n",
    "print(\"Root mean squared error: %.2f\"\n",
    "      % math.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print('percentage absolute error: %.2f' %mean(abs((y_test - y_pred)/y_test)))\n",
    "print('percentage absolute error: %.2f' %(mean(abs(y_test - y_pred))/mean(y_test)))\n",
    "print('R-squared: %.2f' % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "coefficients = np.column_stack((X.columns, regr.coef_)) #join column names and coeffcients\n",
    "print('Coefficients: \\n', coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#plot predicted values vs residual\n",
    "#use blue for residuals of training data\n",
    "plt.scatter(regr.predict(x_train), y_train - regr.predict(x_train), c='b') \n",
    "#use green for residuals of test data\n",
    "plt.scatter(y_test, y_test - y_pred, c='g') \n",
    "plt.ylabel('residuals')\n",
    "plt.xlabel('actual value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we use recursive feature elimination to select the best model with 5 features.<br> \n",
    "the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n",
    "<a href='http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE'>more information</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to normalize data before using RFE\n",
    "\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "x_train_normalized = pd.DataFrame(scaler.transform(x_train), columns = x_train.columns)\n",
    "x_test_normalized = pd.DataFrame(scaler.transform(x_test), columns = x_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "regr = LinearRegression()\n",
    "#parameters: estimator, n_features_to_select=None, step=1\n",
    "selector = RFE(regr, 5, step=1) \n",
    "selector.fit(x_train_normalized, y_train)\n",
    "selector.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_best_model(support_array, columns, model, test_data, test_labels):\n",
    "    y_pred = model.predict(test_data.iloc[:, support_array])\n",
    "    r2 = r2_score(test_labels, y_pred)\n",
    "    n = len(y_pred) #size of test set\n",
    "    p = len(model.coef_) #number of features\n",
    "    adjusted_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "    print('Adjusted R-squared: %.2f' % adjusted_r2)\n",
    "    j = 0;\n",
    "    for i in range(len(support_array)):\n",
    "        if support_array[i] == True:\n",
    "            print(columns[i], model.coef_[j])\n",
    "            j +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_best_model(selector.support_, x_train.columns, selector.estimator_, x_test_normalized, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>Assignment: Write code that examines feature sets of size 1 to 10 and compares their adjusted r-squared values using a for loop. Select the best regression model based on the improvement made to the r-squared. You can ignore any improvement <= 1%. After finding the best model, run it again using original (not normalized) data. Submit your notebook containing your code and the best model with unnormalized coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
